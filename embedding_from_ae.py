# -*- coding: utf-8 -*-
"""embedding_from_ae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXHZ-Iqa4LYYTnr4FRBSoP6RpSuTZcud
"""

from tqdm import tqdm
from auto_encoder import AutoEncoder
from masked_dataset import CIFAR10DataModule
import torch
from torch import nn
from torch.nn import functional as F

batch_size = 4
cifar10 = CIFAR10DataModule(batch_size=batch_size)
cifar10.setup("nothing")
cifar10_train = cifar10.train_dataloader()

in_channel_size = 3
num_epochs = 10
model = AutoEncoder(in_channel_size=3).cuda()
model.eval()

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
checkpoint = torch.load(
    "lightning_logs/version_1/checkpoints/last.ckpt", map_location=device
)
model.load_state_dict(checkpoint["state_dict"])

original_size = 224 * 224

mseloss = nn.MSELoss(reduction="none")
weights = torch.zeros([original_size, original_size], dtype=torch.float16)
times_this_index_is_sampled = torch.zeros(original_size)

for batch_idx, (masked, full, indices) in tqdm(enumerate(cifar10_train)):
    masked = masked.cuda()
    full = full.cuda()
    # pass the sampled data to the model
    prediction = model(masked)
    # go through each image in the current batch
    for i in range(batch_size):
        # get loss per pixel across all channels
        loss = mseloss(prediction[i], full[i])
        single_loss = torch.sum(loss, dim=0)
        # get the indices that were in the sample
        idxs = indices[i]
        # add the losses to the index's loss tracker
        # iterate the number of samples seen
        weights[idxs] += single_loss.cpu()
        times_this_index_is_sampled[idxs] += 1

torch.save(weights, "weights.pth")

# total loss / number of times seen
weights /= times_this_index_is_sampled

torch.save(weights, "divided.pth")

# normalize across rows
weights = F.normalize(weights, dim=0)

torch.save(weights, "embedding_tensor.pth")
